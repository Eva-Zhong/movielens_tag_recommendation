{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train word2vec using imdb_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes are based on Yuan's codes that train doc2vec on imdb_review. See manage/pyml4/expt/gensim/doc2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from os.path import dirname\n",
    "import os,sys,inspect\n",
    "\n",
    "current_dir = dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "manage_dir = dirname(dirname(dirname(current_dir)))\n",
    "\n",
    "if not manage_dir in sys.path:\n",
    "    sys.path.insert(0, manage_dir)\n",
    "\n",
    "# from pyml4.common import context, db\n",
    "# context.local_context.print_summary()\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import logging\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "import pymysql.cursors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/evazhong/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "sent_detector = None\n",
    "\n",
    "# store NLTK resources inside ML4\n",
    "#nltk_download_dir = os.path.dirname(nltk.__file__)\n",
    "#nltk.data.path.append(os.path.dirname(nltk.__file__))\n",
    "#nltk.download('punkt', download_dir=os.path.dirname(nltk.__file__))\n",
    "nltk.download('punkt')\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "NUM_REVIEWS_TO_FETCH = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewSentences(object):\n",
    "    # def __init__(self):\n",
    "    #     self.db_client = db.manually_connect()\n",
    "    #\n",
    "    # def __del__(self):\n",
    "    #     self.db_client.close()\n",
    "\n",
    "    def __iter__(self):\n",
    "        connection = pymysql.connect(host=\"127.0.0.1\",\n",
    "                                    user=\"web\",\n",
    "                                    password=\"atth1132\",\n",
    "                                    db=\"ML3_mirror\")\n",
    "\n",
    "        with connection.cursor() as cur:\n",
    "        #with self.db_client.cursor() as cur:\n",
    "        #cur.execute(\"select imdbReviewId, movieId, body, upVotes, downVotes from imdb_review \")\n",
    "            cur.execute(\"\"\"select imdbReviewId, movieId, body, upVotes, downVotes\n",
    "                            FROM (select imdbReviewId, movieId, body, upVotes, downVotes,\n",
    "                            @rn:= if(@movieId = movieId, @rn+1, 1) as rn,\n",
    "                            @movieId:=movieId as pi\n",
    "                            from imdb_review, (select @rn:=0) b\n",
    "                            order by movieId, (upVotes-downVotes)desc) x\n",
    "                            where x.rn <= 80 and x.pi < 5500\n",
    "                            \"\"\")\n",
    "            for (review_id, movie_id, body, up, down) in cur:\n",
    "                for sentence in sent_detector.tokenize(body):\n",
    "                    yield (movie_id, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sentences = [(movie_id,sentence) for (movie_id,sentence) in ReviewSentences()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/evazhong/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "{'ma', 'myself', \"doesn't\", 'my', 'and', 'haven', 'doing', \"it's\", 'on', 'shouldn', 'off', \"wouldn't\", 'most', 'ourselves', 'of', \"you'll\", 'because', 'here', \"hasn't\", \"don't\", 'them', \"haven't\", 'by', 'aren', 'than', 'both', 'itself', 'y', 'can', 'do', 'be', \"she's\", 's', 'hers', 'will', 'just', 'should', 'now', 'under', 'wouldn', 'her', 'when', 'to', 'only', \"weren't\", 'been', 'who', 'you', 'very', \"didn't\", 'm', 'won', 'over', 'how', 'isn', \"you're\", 'being', 'that', \"that'll\", 'has', 'its', 'wasn', \"wasn't\", 'your', 'an', 'during', 'couldn', 'through', 'up', 'nor', 'their', 'all', 'at', 'which', 'did', 'what', 'with', 'these', \"you'd\", 'we', 've', \"shan't\", 'mightn', \"you've\", 'the', 'from', 'where', 'our', \"isn't\", 'once', \"needn't\", 'while', 'between', 'don', 'does', 'few', 'for', 'further', 'about', 'below', \"hadn't\", \"couldn't\", 'into', 'no', 'again', 'some', 'weren', 'or', 're', 'herself', 'needn', 'yours', 'o', 'as', 'why', 'have', 'those', 'in', 'll', 'such', 'doesn', 'ain', 'hadn', 'i', 'they', 'themselves', 'whom', 'this', 'was', 'any', \"mightn't\", 'too', 'same', 'shan', \"aren't\", \"shouldn't\", 'yourself', 'his', 'didn', 'me', 'he', 'out', 'own', 'hasn', 'had', 'd', 'before', 't', 'against', 'not', 'there', 'him', 'were', 'each', 'more', 'am', 'above', 'it', 'mustn', \"should've\", 'so', 'down', 'is', 'ours', 'then', 'having', \"mustn't\", 'but', 'yourselves', 'himself', 'are', 'she', 'until', 'theirs', 'if', 'after', 'a', 'other', \"won't\"}\n"
     ]
    }
   ],
   "source": [
    "#Remove punctuations\n",
    "#nltk.download('stopwords')\n",
    "nltk.download('stopwords')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/evazhong/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "finish lemmatizing\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# stem and remove stopwords from reviews\n",
    "review_sent = []\n",
    "only_sentence = []\n",
    "review_word = []\n",
    "#\n",
    "nltk.download('wordnet')\n",
    "\n",
    "for i in review_sentences:\n",
    "    #print(\"Hi\")\n",
    "    #print(i)\n",
    "    word_lst = [lemma.lemmatize(word,pos='v') for word in tokenizer.tokenize(i[1].lower()) if (word not in stop_words)]\n",
    "    review_word.append((i[0],word_lst))\n",
    "    sent = ' '.join(word_lst)\n",
    "    review_sent.append((i[0],sent))\n",
    "    only_sentence.append(sent)\n",
    "print('finish lemmatizing')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = []\n",
    "for movieId, sentence in review_word:\n",
    "    sentence_list.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the sentences from imdb_review in a csv_file, so it's easier to reuse it.\n",
    "\n",
    "import csv\n",
    "with open('imdb_review_sentences.csv', 'w') as myfile:\n",
    "#     wr = csv.writer(myfile, delimiter=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "#     wr.writerow(sentence_list)\n",
    "    \n",
    "# with open(csvfile, \"w\") as output:\n",
    "    writer = csv.writer(myfile, lineterminator='\\n')\n",
    "    for val in sentence_list:\n",
    "        writer.writerow([val])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that trianing the model can take from 30 mintues to a few hours. \n",
    "An example of the model can be found here: https://drive.google.com/drive/folders/1EBqxmAPAVHxgtaz32PUp0xXsF98B3-GP?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training model2\n",
      "second model trained\n"
     ]
    }
   ],
   "source": [
    "print('start training model2')\n",
    "word_vec_model2 = gensim.models.Word2Vec(sentence_list, alpha=0.025, size=1000, min_alpha=0.025,min_count=10,workers=35,iter=20)\n",
    "#word_vec_model1.train(only_sentence,total_examples=word_vec_model1.corpus_count,epochs=word_vec_model1.iter)\n",
    "word_vec_model2.save('Word2Vec_fullds_model2')\n",
    "print('second model trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65012"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vec_model2.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('happier', 0.27740973234176636),\n",
       " ('happily', 0.2672647535800934),\n",
       " ('happiest', 0.22986197471618652),\n",
       " ('upbeat', 0.22156524658203125),\n",
       " ('glad', 0.20897459983825684),\n",
       " ('optimistic', 0.20554432272911072),\n",
       " ('hopeful', 0.20425133407115936),\n",
       " ('unhappy', 0.19973137974739075),\n",
       " ('cheerful', 0.19736379384994507),\n",
       " ('awww', 0.18560005724430084)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec_model2.wv.similar_by_word(\"happy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word2vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_model2 = Word2Vec.load('Word2Vec_fullds_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65012"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vec_model2.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conclusion', 0.29712265729904175),\n",
       " ('finale', 0.2874034345149994),\n",
       " ('climax', 0.2586871087551117),\n",
       " ('denouement', 0.25783979892730713),\n",
       " ('resolution', 0.2184992879629135),\n",
       " ('dénouement', 0.21513140201568604),\n",
       " ('outcome', 0.21137632429599762),\n",
       " ('coda', 0.2093307375907898),\n",
       " ('scene', 0.2077297866344452),\n",
       " ('begin', 0.20636282861232758),\n",
       " ('final', 0.19717377424240112),\n",
       " ('finalé', 0.19580122828483582),\n",
       " ('resolutions', 0.19251397252082825),\n",
       " ('conclude', 0.1850592941045761),\n",
       " ('closure', 0.18355096876621246),\n",
       " ('finish', 0.1759783774614334),\n",
       " ('start', 0.1751575767993927),\n",
       " ('completion', 0.17112597823143005),\n",
       " ('denouements', 0.16981804370880127),\n",
       " ('epilogue', 0.16950276494026184)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec_model2.wv.similar_by_word(\"end\",topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
